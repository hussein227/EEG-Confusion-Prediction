# -*- coding: utf-8 -*-
"""confusion prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xPNYLHelGnieE29jduJvMop-dGbGzdAv
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import tensorflow as tf

from sklearn.metrics import confusion_matrix, classification_report

# Commented out IPython magic to ensure Python compatibility.
sns.set(style='darkgrid', color_codes=True)
# %matplotlib inline

eeg_df = pd.read_csv('/content/EEG_data.csv')
info_df = pd.read_csv('/content/demographic_info.csv')

eeg_df

info_df

info_df.rename(columns={'subject ID': 'SubjectID'}, inplace=True)

data = info_df.merge(eeg_df, on='SubjectID')

data

data = data.drop(['SubjectID', 'VideoID', 'predefinedlabel'], axis=1)

data.columns

data.rename(columns={' age': 'Age', ' ethnicity': 'Ethnicity', ' gender': 'Gender', 'user-definedlabeln': 'Label'}, inplace=True)

data['Label'] = data['Label'].astype(np.int)

data

print("Missing values:", data.isna().sum().sum())

"""**Encoding Features[OHE])**"""

data['Gender'].unique()

data['Gender'] = data['Gender'].apply(lambda x: 1  if x == 'M' else 0)

data['Ethnicity'].unique()

ethnicity_dummies = pd.get_dummies(data['Ethnicity'])
data = pd.concat([data, ethnicity_dummies], axis=1)
data = data.drop('Ethnicity', axis=1)

data

print("Non-numeric columns:", len(data.select_dtypes('object').columns))

"""**EDA**"""

data.dtypes

data

features = data.drop('Label', axis=1).copy()
num_features = len(features.columns)

print("Features:", num_features)

categorical_features = ['Age', 'Gender', 'Bengali', 'English', 'Han Chinese']
continuous_features = ['Attention', 'Mediation', 'Raw', 'Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']

print("Categorical Features:", len(categorical_features))
print("Continuous Features:", len(continuous_features))

"""**BoxPlot :highlight any outliers in the data**"""

features[continuous_features].plot(kind='box', figsize=(15, 15), subplots=True, layout=(3, 4))
plt.show()

features[continuous_features].plot(kind='hist', bins=25, figsize=(15, 12), subplots=True, layout=(3, 4))
plt.show()

plt.figure(figsize=(20, 5))
for feature in categorical_features:
    plt.subplot(1, 5, categorical_features.index(feature) + 1)
    features[feature].value_counts().plot(kind='pie')
plt.show()

plt.figure(figsize=(8, 8))
data['Label'].value_counts().plot(kind='pie', autopct='%.1f%%')
plt.show()

plt.figure(figsize=(20, 20))
sns.pairplot(features[continuous_features])
plt.show()

corr = data.corr()

plt.figure(figsize=(18, 15))
sns.heatmap(corr, annot=True, vmin=-1.0, cmap='mako')
plt.show()

"""**Spliting and Scalling**

The StandardScaler scales each feature by subtracting its mean and dividing by its standard deviation, so that the resulting distribution has zero mean and unit variance
"""

y = data['Label'].copy()
X = data.drop('Label', axis=1).copy()

scaler = StandardScaler()

X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)

"""**Training**

**Adam optimizer is its ability to adapt the learning rate of each weight parameter based on the past gradients of that parameter, **
"""

inputs = tf.keras.Input(shape=(X_train.shape[1]))
x = tf.keras.layers.Dense(256, activation='relu')(inputs)
x = tf.keras.layers.Dense(256, activation='relu')(x)
outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)

model = tf.keras.Model(inputs, outputs)


model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        tf.keras.metrics.AUC(name='auc')
    ]
)

batch_size = 32
epochs = 50

history = model.fit(
    X_train,
    y_train,
    validation_split=0.2,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=[
        tf.keras.callbacks.ReduceLROnPlateau()
    ]
)

"""Training loss is the error or cost function that is minimized during the training process. It represents the difference between the predicted output of the model and the actual output for the training data. The goal of training is to minimize this loss function, which is typically done using gradient descent optimization algorithms.

Validation loss, on the other hand, is the error or cost function calculated on a separate set of data called the validation set, which is not used for training the model. This set is used to evaluate the model's performance on unseen data and to prevent overfitting, which is when the model is too complex and performs well on the training data but poorly on new data.
"""

plt.figure(figsize=(16, 10))

plt.plot(range(epochs), history.history['loss'], label="Training Loss")
plt.plot(range(epochs), history.history['val_loss'], label="Validation Loss")

plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Over Time")
plt.legend()

plt.show()

model.evaluate(X_test, y_test)

y_true = np.array(y_test)

y_pred = np.squeeze(model.predict(X_test))
y_pred = np.array(y_pred >= 0.5, dtype=np.int)

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(4, 4))

sns.heatmap(cm, annot=True, fmt='g', vmin=0, cbar=False)

plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")

plt.show()

print(classification_report(y_true, y_pred))